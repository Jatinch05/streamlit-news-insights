import streamlit as st
import pandas as pd
import subprocess
import os
import sys
import glob
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from io import StringIO
from datetime import datetime
from streamlit_autorefresh import st_autorefresh

# Set layout
st.set_page_config(page_title="News Dashboard", layout="wide")

# Auto-refresh every 10 minutes
st_autorefresh(interval=10 * 60 * 1000, key="refresh")

st.title("üóûÔ∏è News Headlines Dashboard")

# --- UTILS ---

def fetch_latest_news():
    try:
        project_root = os.path.dirname(__file__)
        subprocess.run([
            sys.executable, "src/main.py", "--config", "config.yaml", "--export", "csv"
        ], check=True, cwd=project_root)
        st.success("‚úÖ Fetched latest news! Please reload the dashboard.")
    except subprocess.CalledProcessError as e:
        st.error(f"‚ùå Fetch failed: {e}")

def get_latest_csv():
    files = glob.glob("data/headlines-*.csv")
    if not files:
        return None
    return max(files, key=os.path.getctime)

@st.cache_data(ttl=60)
def load_data(path):
    if os.path.exists(path) and os.path.getsize(path) == 0:
        os.remove(path)
        st.warning("‚ùå Removed empty data file. Please re-run the scraper.")
        st.stop()
    try:
        df = pd.read_csv(path, parse_dates=["published_at"])
        if df.empty:
            st.warning("‚ö†Ô∏è CSV exists but contains no records.")
            st.stop()
        return df
    except pd.errors.EmptyDataError:
        st.warning("‚ö†Ô∏è The data file is empty or malformed.")
        st.stop()

# --- Sentiment Interpretation ---
def interpret_sentiment(score):
    if score <= -0.5:
        return "Very Negative"
    elif score < -0.1:
        return "Negative"
    elif score <= 0.1:
        return "Neutral"
    elif score < 0.5:
        return "Positive"
    else:
        return "Very Positive"

# --- Manual Refresh ---
if st.button("üì° Fetch Latest News"):
    fetch_latest_news()
    st.cache_data.clear()
    st.rerun()

# --- Load Data ---
data_path = os.path.join(os.path.dirname(__file__), "data/headlines-latest.csv")

if not data_path or not os.path.exists(data_path):
    st.warning("‚ö†Ô∏è No data file found. Please fetch the latest news first.")
    st.stop()

df = load_data(data_path)
df["published_at"] = pd.to_datetime(df["published_at"], errors="coerce")
df = df.dropna(subset=["published_at"])
st.caption(f"üìÅ Loaded: `{data_path}`")

# --- Filters ---
st.sidebar.header("Filters")

min_date = df["published_at"].dt.date.min()
max_date = df["published_at"].dt.date.max()
st.caption(f"üóìÔ∏è Data available from **{min_date}** to **{max_date}**")

sources = st.sidebar.multiselect(
    "Filter by source", options=df["source"].unique(), default=df["source"].unique()
)

date_range = st.sidebar.date_input(
    "Date range",
    value=[min_date, max_date],
    min_value=min_date,
    max_value=max_date
)

query = st.sidebar.text_input("üîç Search in headlines", "")

# --- Apply Filters ---
mask = (
    df["source"].isin(sources) &
    df["published_at"].dt.date.between(*date_range)
)
filtered = df.loc[mask]

if query:
    filtered = filtered[filtered["headline"].str.contains(query, case=False, na=False)]

if filtered.empty:
    st.warning("‚ö†Ô∏è No articles match the current filters.")
    st.stop()

# --- Enrich with sentiment and hour ---
filtered["sentiment"] = filtered["headline"].apply(lambda h: TextBlob(h).sentiment.polarity)
filtered["hour"] = filtered["published_at"].dt.hour
filtered["headline_link"] = filtered.apply(
    lambda row: f"[{row['headline']}]({row['url']})", axis=1
)

# --- Summary Stats ---
st.subheader("üìä Summary")
col1, col2, col3 = st.columns(3)

avg_sentiment = filtered["sentiment"].mean()
sentiment_label = interpret_sentiment(avg_sentiment)

col1.metric("üì∞ Total Articles", len(filtered))
col2.metric("üß† Avg Sentiment", f"{avg_sentiment:.2f}", sentiment_label)
col3.metric("‚è∞ Sources", ", ".join(filtered["source"].unique()))

# --- Export filtered results ---
csv_buffer = StringIO()
filtered.to_csv(csv_buffer, index=False)
st.download_button(
    label="üì• Download Filtered Headlines",
    data=csv_buffer.getvalue(),
    file_name=f"filtered_{datetime.utcnow().strftime('%Y%m%d_%H%M')}.csv",
    mime="text/csv"
)

# --- Visualizations ---
st.subheader("üìà Articles Over Time")
ts = filtered.groupby(filtered["published_at"].dt.date).size()
st.line_chart(ts)

st.subheader("üìä Articles by Hour of Day")
hourly = filtered.groupby("hour").size()
st.bar_chart(hourly)

st.subheader("üìä Sentiment by Source")
sent_chart = filtered.groupby("source")["sentiment"].mean()
st.bar_chart(sent_chart)

st.subheader("üìë Articles per Source")
st.dataframe(
    filtered["source"]
    .value_counts()
    .reset_index()
    .rename(columns={"index": "source", "source": "Count"})
)

# --- Word Cloud ---
st.subheader("‚òÅÔ∏è Most Common Words")
from collections import Counter
import re
text = " ".join(filtered["headline"].dropna().tolist())
wordcloud = WordCloud(width=800, height=300, background_color='black').generate(text)
fig, ax = plt.subplots(figsize=(10, 4))
ax.imshow(wordcloud, interpolation="bilinear")
ax.axis("off")
st.pyplot(fig)

# --- Headline Table ---
st.subheader("üì∞ Latest Headlines")
st.dataframe(
    filtered.sort_values("published_at", ascending=False)[
        ["published_at", "source", "headline_link"]
    ].rename(columns={"headline_link": "headline"}),
    use_container_width=True,
    height=500
)
