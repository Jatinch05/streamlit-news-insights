# ğŸ“° News Insights Dashboard

A real-time, sentiment-aware dashboard that scrapes, analyzes, and visualizes news headlines from multiple major sources using asynchronous Python and Streamlit.

---

## ğŸš€ Features

- **Asynchronous news scraping** using `aiohttp` for fast parallel fetches
- **Multi-source support**: BBC, CNN, TechCrunch, The Guardian (and easily extendable)
- **Sentiment analysis** using `TextBlob`
- **Interactive dashboard** built with `Streamlit`
- **Visualizations**: time trends, sentiment breakdowns, word clouds, and more
- **Data export** in CSV format
- **Daily auto-scraping** via GitHub Actions
- **SQLite storage** for persistent historical data

---

## ğŸ“¦ Tech Stack

- **Backend**: Python 3.10, `asyncio`, `aiohttp`, `feedparser`
- **Frontend**: Streamlit
- **NLP**: TextBlob (Sentiment), WordCloud
- **Storage**: SQLite, CSV
- **Automation**: GitHub Actions (daily scraping)

---

## ğŸ“Š Dashboard Preview

![Dashboard Screenshot]("C:\Users\jatin\OneDrive\Pictures\Screenshots\Screenshot 2025-07-08 184701.png")

---

## ğŸ› ï¸ Setup Instructions

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/streamlit-news-insights.git
   cd streamlit-news-insights
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the scraper**
   ```bash
   python src/main.py --config config.yaml --export sqlite
   ```

5. **Launch the dashboard**
   ```bash
   streamlit run src/dashboard.py
   ```

---

## âš™ï¸ GitHub Actions (Auto Scraper)

- Scheduled to run daily at **11 30 AM IST**
- Updates the `headlines.db` file and commits changes to GitHub

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py          # Orchestrates fetch, parse, transform, store
â”‚   â”œâ”€â”€ fetcher.py       # AsyncFetcher class using aiohttp
â”‚   â”œâ”€â”€ parser.py        # RSS parsers for each source
â”‚   â”œâ”€â”€ transformer.py   # Cleans and normalizes raw data
â”‚   â”œâ”€â”€ storage.py       # Stores to SQLite/CSV/Parquet
â”‚   â”œâ”€â”€ dashboard.py     # Streamlit frontend
â”œâ”€â”€ data/                # Database and CSV exports
â”œâ”€â”€ config.yaml          # Site configs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .github/workflows/   # GitHub Actions automation
â””â”€â”€ README.md
```

---

## ğŸ“Œ Future Work

- Add topic classification (e.g., Tech, Politics, Sports)
- Enhance scraping with full-text article parsing
- Upgrade to PostgreSQL for scalable storage


---

## ğŸ“„ License

MIT License â€“ feel free to use, modify, and contribute.